<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>My first three.js app</title>
    <style>
        body {
            margin: 0;
        }
    </style>
</head>

<body>
    <div id="overlay">
        <button id="startButton">Play</button>
    </div>
    <div id="container">
		<button id="playAudioButton">Play Audio</button>
	</div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <script type="module">

		import * as THREE from 'https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.module.js';

		const startButton = document.getElementById('startButton');
        startButton.addEventListener('click', init);
		function init() {
			const overlay = document.getElementById('overlay');
            overlay.remove();

            //
            const container = document.getElementById('container');
            const playAudioButton = document.getElementById('playAudioButton');
            playAudioButton.addEventListener('click', playAudio);
            function playAudio() {
                const listener = new THREE.AudioListener();

            const audio = new THREE.Audio( listener );
            const file = './sounds/drums.mp3';

            if ( /(iPad|iPhone|iPod)/g.test( navigator.userAgent ) ) {

                const loader = new THREE.AudioLoader();
                loader.load( file, function ( buffer ) {

                    audio.setBuffer( buffer );
                    audio.play();

                } );

            } else {

                const mediaElement = new Audio( file );
                mediaElement.play();

                audio.setMediaElementSource( mediaElement );

            };
            }
            analyser = new THREE.AudioAnalyser( audio, fftSize );

    //

    const format = (renderer.capabilities.isWebGL2) ? THREE.RedFormat : THREE.LuminanceFormat;

uniforms = {
    tAudioData: { value: new THREE.DataTexture(analyser.data, analyser.data.length, 1, format) }
};

const scene = new THREE.Scene();
const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);

const renderer = new THREE.WebGLRenderer();
renderer.setSize(window.innerWidth, window.innerHeight);
document.body.appendChild(renderer.domElement);

const vertexShaderCode = `
    varying vec2 vUv;

    void main() {
        vUv = uv;
        vec4 modelViewPosition = modelViewMatrix * vec4(position, 1.0);
        gl_Position = projectionMatrix * modelViewPosition;
    }
`;

const fragmentShaderCode = `
    uniform sampler2D tAudioData;
    varying vec2 vUv;

    void main() {
        vec3 color = texture2D(tAudioData, vec2(vUv.x, 0.5)).rgb;
        gl_FragColor = vec4(color, 1.0);
    }
`;

const material = new THREE.ShaderMaterial({
    uniforms: uniforms,
    vertexShader: vertexShaderCode,
    fragmentShader: fragmentShaderCode
});

const geometry = new THREE.PlaneGeometry(5, 1, analyser.data.length - 1, 1);
const mesh = new THREE.Mesh(geometry, material);
scene.add(mesh);

			camera.position.z = 5;
			function animate() {
			    requestAnimationFrame( animate );

			    analyser.getFrequencyData();

                    // Update the y-position of the vertices based on the audio data
                    for (let i = 0; i < analyser.data.length; i++) {
                        mesh.geometry.vertices[i].y = analyser.data[i] / 100; // Adjust the scale as needed
                    }

                    mesh.geometry.verticesNeedUpdate = true;

			    renderer.render( scene, camera );
		    }
			animate();
		}
		

		
    </script>
</body>

</html>
